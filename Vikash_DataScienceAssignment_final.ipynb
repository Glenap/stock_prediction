{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b2497f",
   "metadata": {},
   "source": [
    "# Predicting Stock Price Movements — Final Notebook\n",
    "\n",
    "**File:** `Vikash_DataScienceAssignment_final.ipynb`\n",
    "\n",
    "This notebook includes data collection, cleaning, feature engineering, baseline models (Logistic, RF, XGB), enhanced features, hyperparameter tuning, LightGBM, and an ensemble (RF+XGB+LGBM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c71df1",
   "metadata": {},
   "source": [
    "## 0. Setup — Install required libraries\n",
    "Run this cell once. If running on Colab/Kaggle some packages may already be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd07adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet yfinance pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm ta joblib shap mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c944b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and plotting style\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, roc_curve, auc, classification_report,\n",
    "                             matthews_corrcoef)\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "import ta\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "# Use a valid matplotlib style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except Exception:\n",
    "    plt.style.use('default')\n",
    "\n",
    "%matplotlib inline\n",
    "print('Imports done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7051d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data collection\n",
    "TICKER = 'HDFCBANK.NS'  # change as needed\n",
    "print('Ticker:', TICKER)\n",
    "\n",
    "data = yf.download(TICKER, period='2y', interval='1d', auto_adjust=False)\n",
    "# keep relevant columns\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    # sometimes yfinance returns single-level, sometimes multi; collapse if needed\n",
    "    try:\n",
    "        data.columns = data.columns.droplevel(1)\n",
    "    except Exception:\n",
    "        data.columns = data.columns.get_level_values(0)\n",
    "\n",
    "data = data[['Open','High','Low','Close','Adj Close','Volume']].dropna()\n",
    "# chronological order\n",
    "data = data.sort_index()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73428df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data cleaning\n",
    "# Forward/backfill\n",
    "data = data.ffill().bfill()\n",
    "\n",
    "# Robust outlier capping for Volume\n",
    "Q1 = data['Volume'].quantile(0.25)\n",
    "Q3 = data['Volume'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "if pd.isna(Q1) or pd.isna(Q3) or pd.isna(IQR):\n",
    "    print('Volume quantiles are NaN; skipping cap.')\n",
    "else:\n",
    "    lower = float(Q1 - 1.5 * IQR)\n",
    "    upper = float(Q3 + 1.5 * IQR)\n",
    "    if lower >= upper:\n",
    "        print('Degenerate IQR bounds; skipping cap.')\n",
    "    else:\n",
    "        data['Volume'] = data['Volume'].clip(lower=lower, upper=upper)\n",
    "        print(f'Capped Volume to range [{lower:.2f}, {upper:.2f}]')\n",
    "\n",
    "# compute daily returns\n",
    "data['Return'] = data['Adj Close'].pct_change()\n",
    "\n",
    "# handle zero-volume rows by replacing 0 with NaN then ffill\n",
    "if (data['Volume'] == 0).any():\n",
    "    data.loc[data['Volume'] == 0, 'Volume'] = np.nan\n",
    "    data['Volume'] = data['Volume'].ffill().bfill()\n",
    "\n",
    "# drop initial NaNs\n",
    "data = data.dropna()\n",
    "\n",
    "print('Data shape after cleaning:', data.shape)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04964d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature engineering\n",
    "# Ensure Adj Close is Series\n",
    "adj = data['Adj Close']\n",
    "\n",
    "# Moving averages and momentum indicators\n",
    "data['SMA_20'] = adj.rolling(20).mean()\n",
    "data['EMA_20'] = adj.ewm(span=20, adjust=False).mean()\n",
    "\n",
    "# MACD\n",
    "ema12 = adj.ewm(span=12, adjust=False).mean()\n",
    "ema26 = adj.ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = ema12 - ema26\n",
    "data['MACD_signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# RSI\n",
    "data['RSI_14'] = ta.momentum.rsi(adj, window=14)\n",
    "\n",
    "# Bollinger Bands\n",
    "bb = ta.volatility.BollingerBands(close=adj, window=20, window_dev=2)\n",
    "data['BB_high'] = bb.bollinger_hband()\n",
    "data['BB_low'] = bb.bollinger_lband()\n",
    "data['BB_width'] = data['BB_high'] - data['BB_low']\n",
    "\n",
    "# Extra engineered features\n",
    "for lag in range(1,6):\n",
    "    data[f'return_lag_{lag}'] = adj.pct_change(lag)\n",
    "\n",
    "data['mean_ret_5'] = adj.pct_change().rolling(5).mean()\n",
    "data['mean_ret_10'] = adj.pct_change().rolling(10).mean()\n",
    "data['vol_10'] = adj.pct_change().rolling(10).std()\n",
    "data['vol_20'] = adj.pct_change().rolling(20).std()\n",
    "\n",
    "data['close_open_ratio'] = data['Close'] / data['Open'] - 1\n",
    "data['high_low_ratio'] = data['High'] / data['Low'] - 1\n",
    "data['vol_change'] = data['Volume'].pct_change()\n",
    "\n",
    "# TA extras\n",
    "data['ADX_14'] = ta.trend.adx(data['High'], data['Low'], adj, window=14)\n",
    "data['OBV'] = ta.volume.on_balance_volume(adj, data['Volume'])\n",
    "\n",
    "# Weekday\n",
    "data['dayofweek'] = data.index.dayofweek\n",
    "\n",
    "# Drop NaNs introduced by indicators\n",
    "data = data.dropna()\n",
    "print('Data shape after feature engineering:', data.shape)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed1410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Target construction (smoothed 3-day)\n",
    "data['future_mean_3d'] = data['Adj Close'].shift(-1).rolling(window=3).mean()\n",
    "data['Target_3d'] = (data['future_mean_3d'] > data['Adj Close']).astype(int)\n",
    "# drop rows with NaN in target\n",
    "data = data.dropna()\n",
    "print('Target distribution (Target_3d):')\n",
    "print(data['Target_3d'].value_counts(normalize=True))\n",
    "\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece671e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Exploratory Data Analysis (quick)\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(data.index, data['Adj Close'], label='Adj Close')\n",
    "plt.plot(data.index, data['SMA_20'], label='SMA_20')\n",
    "plt.plot(data.index, data['EMA_20'], label='EMA_20')\n",
    "plt.title(f\"{TICKER} Price with Moving Averages\")\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(14,3))\n",
    "plt.plot(data.index, data['RSI_14']); plt.axhline(70, linestyle='--'); plt.axhline(30, linestyle='--'); plt.title('RSI (14)'); plt.show()\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(data.index, data['Adj Close'], label='Adj Close')\n",
    "plt.plot(data.index, data['BB_high'], alpha=0.6); plt.plot(data.index, data['BB_low'], alpha=0.6);\n",
    "plt.fill_between(data.index, data['BB_low'], data['BB_high'], alpha=0.1); plt.title('Bollinger Bands'); plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(14,4))\n",
    "axs[0].plot(data.index, data['Volume']); axs[0].set_title('Volume')\n",
    "axs[1].hist(data['Return'].dropna(), bins=50); axs[1].set_title('Daily Return Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Prepare features and time-based split\n",
    "feature_cols = [\n",
    "    'SMA_20','EMA_20','RSI_14','MACD','MACD_signal','BB_width',\n",
    "    'return_lag_1','return_lag_2','return_lag_3','return_lag_4','return_lag_5',\n",
    "    'mean_ret_5','mean_ret_10','vol_10','vol_20','close_open_ratio','high_low_ratio',\n",
    "    'vol_change','ADX_14','OBV','dayofweek','Volume'\n",
    "]\n",
    "feature_cols = [c for c in feature_cols if c in data.columns]\n",
    "print('Using features:', feature_cols)\n",
    "\n",
    "X = data[feature_cols].copy()\n",
    "y = data['Target_3d'].copy()\n",
    "\n",
    "# scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), index=X.index, columns=X.columns)\n",
    "\n",
    "# time-based split 80/20\n",
    "split_idx = int(len(X_scaled) * 0.8)\n",
    "X_train = X_scaled.iloc[:split_idx]\n",
    "X_test = X_scaled.iloc[split_idx:]\n",
    "y_train = y.iloc[:split_idx]\n",
    "y_test = y.iloc[split_idx:]\n",
    "\n",
    "print('Train size:', X_train.shape, 'Test size:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c37614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Baseline models: Logistic, RandomForest, XGBoost\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "fitted = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    fitted[name] = model\n",
    "    print(f'{name} trained')\n",
    "\n",
    "# Evaluate baselines\n",
    "results = {}\n",
    "for name, model in fitted.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test)[:,1]\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = model.decision_function(X_test)\n",
    "            y_proba = (df - df.min()) / (df.max() - df.min())\n",
    "        except Exception:\n",
    "            y_proba = np.zeros(len(y_pred))\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    results[name] = {'accuracy':acc,'precision':prec,'recall':rec,'f1':f1,'roc_auc':roc_auc,'confusion_matrix':cm}\n",
    "\n",
    "pd.DataFrame({k:{'accuracy':v['accuracy'],'precision':v['precision'],'recall':v['recall'],'f1':v['f1'],'roc_auc':v['roc_auc']} for k,v in results.items()}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Enhanced hyperparameter tuning (RandomizedSearchCV) for RF and XGB\n",
    "# TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "# RandomForest search\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rf_param_dist = {\n",
    "    'n_estimators': randint(100, 400),\n",
    "    'max_depth': randint(3, 20),\n",
    "    'min_samples_split': randint(2, 12),\n",
    "    'min_samples_leaf': randint(1, 6),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "rf_search = RandomizedSearchCV(rf, rf_param_dist, n_iter=20, scoring='f1', cv=tscv, random_state=42, n_jobs=-1)\n",
    "rf_search.fit(X_train, y_train)\n",
    "best_rf = rf_search.best_estimator_\n",
    "print('Best RF params:', rf_search.best_params_)\n",
    "\n",
    "# XGBoost search\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "xgb_param_dist = {\n",
    "    'n_estimators': randint(80, 400),\n",
    "    'max_depth': randint(2, 12),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'colsample_bytree': uniform(0.5, 0.5)\n",
    "}\n",
    "xgb_search = RandomizedSearchCV(xgb, xgb_param_dist, n_iter=20, scoring='f1', cv=tscv, random_state=42, n_jobs=-1)\n",
    "xgb_search.fit(X_train, y_train)\n",
    "best_xgb = xgb_search.best_estimator_\n",
    "print('Best XGB params:', xgb_search.best_params_)\n",
    "\n",
    "# Fit on full selected features\n",
    "best_rf.fit(X_train, y_train)\n",
    "best_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Save\n",
    "joblib.dump(best_rf, 'best_rf.pkl')\n",
    "joblib.dump(best_xgb, 'best_xgb.pkl')\n",
    "print('Saved best_rf.pkl and best_xgb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. LightGBM hyperparameter search and fit\n",
    "from lightgbm import LGBMClassifier\n",
    "lgb = LGBMClassifier(objective='binary', random_state=42, n_jobs=-1)\n",
    "param_dist = {\n",
    "    'n_estimators': randint(80, 600),\n",
    "    'num_leaves': randint(15, 255),\n",
    "    'max_depth': randint(3, 16),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'colsample_bytree': uniform(0.5, 0.5),\n",
    "    'reg_alpha': uniform(0.0, 1.0),\n",
    "    'reg_lambda': uniform(0.0, 1.0)\n",
    "}\n",
    "rand_search = RandomizedSearchCV(lgb, param_dist, n_iter=30, scoring='f1', cv=tscv, random_state=42, n_jobs=-1, verbose=1)\n",
    "print('Fitting LightGBM randomized search...')\n",
    "rand_search.fit(X_train, y_train)\n",
    "best_lgb = rand_search.best_estimator_\n",
    "print('Best LGB params:', rand_search.best_params_)\n",
    "\n",
    "# Fit and save\n",
    "best_lgb.fit(X_train, y_train)\n",
    "joblib.dump(best_lgb, 'best_lgbm_model.pkl')\n",
    "print('Saved best_lgbm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f8716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Ensemble RF + XGB + LGBM (probability average)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc)\n",
    "\n",
    "# Load models\n",
    "best_rf = joblib.load('best_rf.pkl')\n",
    "best_xgb = joblib.load('best_xgb.pkl')\n",
    "best_lgb = joblib.load('best_lgbm_model.pkl')\n",
    "models = {'RandomForest':best_rf, 'XGBoost':best_xgb, 'LightGBM':best_lgb}\n",
    "\n",
    "# Ensure selected features\n",
    "try:\n",
    "    selected_features\n",
    "except NameError:\n",
    "    selected_features = X_train.columns.tolist()\n",
    "\n",
    "X_test_sel = X_test[selected_features]\n",
    "\n",
    "# get probas\n",
    "def get_proba(model, X):\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        return model.predict_proba(X)[:,1]\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        df = model.decision_function(X)\n",
    "        return (df - df.min())/(df.max() - df.min())\n",
    "    else:\n",
    "        return model.predict(X).astype(float)\n",
    "\n",
    "probas = {name: get_proba(m, X_test_sel) for name,m in models.items()}\n",
    "# ensemble avg\n",
    "proba_matrix = np.vstack(list(probas.values()))\n",
    "ensemble_proba = proba_matrix.mean(axis=0)\n",
    "# threshold tuning on last part of train\n",
    "val_frac = 0.2\n",
    "val_idx = int(len(X_train)*(1-val_frac))\n",
    "X_val = X_train.iloc[val_idx:]\n",
    "y_val = y_train.iloc[val_idx:]\n",
    "val_probas = [get_proba(m, X_val[selected_features]) for m in models.values()]\n",
    "val_ensemble = np.mean(np.vstack(val_probas), axis=0)\n",
    "thresholds = np.linspace(0.3,0.7,41)\n",
    "best_t=0.5; best_f1=-1\n",
    "from sklearn.metrics import f1_score\n",
    "for t in thresholds:\n",
    "    f = f1_score(y_val, (val_ensemble>=t).astype(int), zero_division=0)\n",
    "    if f>best_f1:\n",
    "        best_f1=f; best_t=t\n",
    "print('Best threshold on val:', best_t, 'f1:', best_f1)\n",
    "\n",
    "y_pred_ens = (ensemble_proba>=best_t).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "print('Ensemble Evaluation:')\n",
    "print('Accuracy', accuracy_score(y_test, y_pred_ens))\n",
    "print('Precision', precision_score(y_test, y_pred_ens, zero_division=0))\n",
    "print('Recall', recall_score(y_test, y_pred_ens, zero_division=0))\n",
    "print('F1', f1_score(y_test, y_pred_ens, zero_division=0))\n",
    "print('MCC', matthews_corrcoef(y_test, y_pred_ens))\n",
    "print('ROC AUC', auc(*roc_curve(y_test, ensemble_proba)[:2]))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test, y_pred_ens))\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, y_pred_ens, zero_division=0))\n",
    "\n",
    "# Save ensemble probs\n",
    "out = pd.DataFrame({'date': X_test.index, 'y_true': y_test.values, 'ensemble_proba': ensemble_proba}).set_index('date')\n",
    "out.to_csv('ensemble_probs.csv')\n",
    "print('Saved ensemble_probs.csv')\n",
    "\n",
    "# Feature importances averaged\n",
    "imps = {}\n",
    "for name,m in models.items():\n",
    "    if hasattr(m, 'feature_importances_'):\n",
    "        imps[name] = pd.Series(m.feature_importances_, index=selected_features)\n",
    "if imps:\n",
    "    df_imps = pd.DataFrame(imps).fillna(0)\n",
    "    df_imps['mean_importance'] = df_imps.mean(axis=1)\n",
    "    print('Top combined features:')\n",
    "    display(df_imps['mean_importance'].sort_values(ascending=False).head(15))\n",
    "\n",
    "print('Ensemble complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Save final artifacts (models already saved above)\n",
    "print('Files in working dir:')\n",
    "import os\n",
    "for f in os.listdir('.'):\n",
    "    if f.endswith('.pkl') or f.endswith('.csv') or f.endswith('.ipynb'):\n",
    "        print('-', f)\n",
    "\n",
    "# Save notebook copy\n",
    "import nbformat as nbf\n",
    "nb = nbf.read('Vikash_DataScienceAssignment_final.ipynb', as_version=4)\n",
    "# already the current file; nothing else to do\n",
    "print('Notebook saved as Vikash_DataScienceAssignment_final.ipynb')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
